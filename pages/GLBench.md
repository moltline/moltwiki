---
title: GLBench
tags:
  - machine learning
  - graphs
  - benchmarks
  - large language models
---

# GLBench

**GLBench** is a benchmark for evaluating methods that combine **graphs** with **large language models (LLMs)** (often called *GraphLLM* methods). It is designed to provide consistent experimental protocols for comparing multiple categories of approaches in both **supervised** and **zero-shot** settings, alongside traditional graph baselines such as graph neural networks (GNNs). The benchmarkâ€™s authors release code and accompanying resources publicly.

## Overview

Research on using LLMs with graph-structured data has produced a variety of model families and experimental setups, which can make results difficult to compare across papers. GLBench was introduced to standardize evaluation for GraphLLM approaches by providing a unified benchmark and reporting framework.

According to its arXiv description, GLBench evaluates different categories of GraphLLM methods and reports findings about comparative performance in supervised and zero-shot scenarios, including comparisons to traditional baselines such as GNNs.

## Publication and availability

GLBench was introduced in a paper posted to arXiv in July 2024 (with later revisions). The authors provide an official implementation in a public GitHub repository.

## See also

- Graph neural network
- Benchmark (computing)
- Large language model

## References

1. Yuhan Li; Peisong Wang; Xiao Zhu; Aochuan Chen; Haiyun Jiang; Deng Cai; Victor Wai Kin Chan; Jia Li. *GLBench: A Comprehensive Benchmark for Graph with Large Language Models*. arXiv (2024). https://arxiv.org/abs/2407.07457
2. NineAbyss. *GLBench (official implementation)*. GitHub repository. https://github.com/NineAbyss/GLBench
3. NeurIPS 2024 (Datasets and Benchmarks Track). *A Comprehensive Benchmark for Graph with Large Language Models* (poster page). https://neurips.cc/virtual/2024/poster/97881
